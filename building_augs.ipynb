{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "torch.use_deterministic_algorithms(True)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the batch x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in data ------------------------------------------------------------\n",
    "import My_Anom_extargs as args\n",
    "sys.path.insert(1, '/remote/gpu05/rueschkamp/projects/torch_datasets/')\n",
    "from semi_dataset import SemiV_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#starting training loader --------------------------------------\n",
    "\n",
    "\n",
    "training_set = SemiV_Dataset(\n",
    "                                    data_path = args.data_path,\n",
    "                                    signal_origin= \"qcd\",\n",
    "                                    usage= \"training\",\n",
    "                                    number_constit= args.n_constit,\n",
    "                                    number_of_jets= 1e3,\n",
    "                                    ratio=0.2\n",
    "                                    )\n",
    "\n",
    "dl_training = DataLoader(training_set,batch_size=128, shuffle=True)\n",
    "\n",
    "for i,(data,labels) in enumerate(dl_training):\n",
    "\n",
    "    x = data\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.0063e+01,  3.6949e+01,  1.6779e+01,  1.2583e+01,  1.0393e+01,\n",
      "           3.0307e+00,  2.8600e+00,  2.8419e+00,  2.3515e+00,  2.0964e+00,\n",
      "           1.7141e+00,  1.7024e+00,  1.3026e+00,  1.2009e+00,  1.1571e+00,\n",
      "           4.4814e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 3.1984e-03, -8.2359e-02,  1.6986e-02, -3.8284e-04, -4.6613e-03,\n",
      "           2.4926e-03,  3.4589e-02,  1.4970e-01,  1.4167e-01,  2.8626e-01,\n",
      "           2.8447e-01, -1.8889e-01,  4.1370e-01,  1.6515e-01,  8.1092e-02,\n",
      "           3.5033e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,\n",
      "           5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01,  5.9804e-01],\n",
      "         [-5.8982e-03,  3.2248e-02, -1.0775e-02, -3.0646e-02, -2.1098e-02,\n",
      "           2.8747e-02, -3.9036e-03,  1.8556e-01, -8.9968e-02, -6.1714e-01,\n",
      "           2.4845e-01,  2.6771e-01, -2.9155e-01,  2.5688e-01,  3.7194e-02,\n",
      "          -6.5485e-03, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00,\n",
      "          -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00, -1.3170e+00]],\n",
      "\n",
      "        [[ 1.9177e+01,  1.5644e+01,  1.3049e+01,  1.3002e+01,  1.2650e+01,\n",
      "           1.2362e+01,  1.1088e+01,  1.1045e+01,  9.2980e+00,  8.8499e+00,\n",
      "           8.1062e+00,  7.7399e+00,  6.8693e+00,  6.8107e+00,  5.9593e+00,\n",
      "           5.6052e+00,  4.9241e+00,  4.7540e+00,  4.7196e+00,  4.5078e+00,\n",
      "           4.4683e+00,  3.8491e+00,  3.7355e+00,  3.3943e+00,  3.0341e+00,\n",
      "           2.9861e+00,  2.9524e+00,  2.8123e+00,  2.7931e+00,  2.7210e+00,\n",
      "           2.6521e+00,  2.5218e+00,  2.2651e+00,  2.2429e+00,  1.9758e+00,\n",
      "           1.9508e+00,  1.9087e+00,  1.7843e+00,  1.7557e+00,  1.7113e+00,\n",
      "           1.6343e+00,  1.6323e+00,  1.4681e+00,  1.4349e+00,  1.4053e+00,\n",
      "           1.3817e+00,  1.2430e+00,  1.2171e+00,  1.2028e+00,  1.1227e+00],\n",
      "         [ 1.8117e-01, -2.9255e-02,  1.7190e-01,  1.6901e-01, -7.3912e-02,\n",
      "          -1.6486e-01, -7.3587e-02, -1.4115e-02,  1.5093e-01,  1.3421e-01,\n",
      "           1.2186e-01, -1.4651e-01,  3.1419e-02, -3.8147e-01,  1.7832e-01,\n",
      "           1.7005e-01,  7.2014e-02,  9.8213e-03, -2.6453e-01,  1.6907e-02,\n",
      "          -1.4359e-01, -1.2007e-01,  1.4579e-02,  1.7029e-01, -1.7401e-03,\n",
      "          -1.6361e-01, -1.7727e-01,  3.5290e-02, -6.1015e-01, -3.5527e-02,\n",
      "           7.9571e-02, -1.1969e-01,  5.0152e-02, -2.5478e-01, -2.3063e-02,\n",
      "           1.9263e-01,  2.1614e-01, -2.3770e-02, -6.2393e-01, -9.5825e-02,\n",
      "           4.1232e-01, -2.6197e-03,  1.7961e-01,  1.3642e-02, -1.0929e-01,\n",
      "          -1.6184e-02, -4.2146e-02, -7.1565e-01,  1.7145e-01, -5.1797e-01],\n",
      "         [-2.2427e+00,  3.9815e+00, -2.2749e+00, -2.1936e+00,  3.9517e+00,\n",
      "          -2.2584e+00, -2.2684e+00, -2.2631e+00, -2.2758e+00, -2.2724e+00,\n",
      "           3.2673e+00, -2.1783e+00,  3.9859e+00, -2.2391e+00, -1.9608e+00,\n",
      "          -2.2576e+00,  3.9414e+00,  3.7728e+00, -2.1309e+00,  3.9560e+00,\n",
      "           4.0025e+00, -2.1383e+00, -2.1263e+00,  3.8935e+00, -2.1375e+00,\n",
      "          -2.2585e+00,  3.8763e+00,  3.9935e+00, -2.1199e+00,  3.7618e+00,\n",
      "           3.3345e+00, -2.1394e+00,  4.0045e+00, -2.1879e+00,  3.9764e+00,\n",
      "          -2.1118e+00,  3.9289e+00,  3.8506e+00,  3.9555e+00,  3.7135e+00,\n",
      "           3.5895e+00,  3.9409e+00, -1.9088e+00, -2.2097e+00, -2.1773e+00,\n",
      "          -1.8096e+00, -2.1470e+00,  3.8010e+00, -1.9989e+00, -2.0346e+00]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.randint(0,20,[128, 3, 50])\n",
    "x = torch.ones([128, 3, 50])*20\n",
    "\n",
    "pTs= torch.ones([128, 50])*10\n",
    "etas = torch.ones([128, 50])*100\n",
    "phis = torch.ones([128, 50])*1000\n",
    "x = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "\n",
    "x = x = torch.ones([1, 3, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Augmentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic fkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_pts(batch):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of pT-rescaled jets, each constituent pT is rescaled by 600, same shape as input\n",
    "    '''\n",
    "    batch_rscl = batch.clone()\n",
    "    batch_rscl[:,0,:] = torch.nan_to_num(batch_rscl[:,0,:]/600, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return batch_rscl\n",
    "\n",
    "def recentre_jet(batch):\n",
    "    batchc = batch.clone()\n",
    "    pts = batch[:,0,:]\n",
    "    etas = batch[:,1,:]\n",
    "    phis = batch[:,2,:]\n",
    "    if torch.sum( pts ) != 0:\n",
    "        eta_shift = torch.sum(  pts*etas  ) / torch.sum( pts )\n",
    "        phi_shift = torch.sum(  pts*phis ) / torch.sum( pts )\n",
    "        etas = etas - eta_shift\n",
    "        phis = phis - phi_shift\n",
    "\n",
    "    pTs, indices = torch.sort(pts, dim=1, descending=True) # Ordering pTs\n",
    "    etas = etas.gather(dim=1,index=indices)\n",
    "    phis = phis.gather(dim=1,index=indices)\n",
    "\n",
    "    batch_recentred = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "\n",
    "    #print(batch_dropped.size())\n",
    "    return batch_recentred\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking recentre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recentre_jet_np(batch):\n",
    "    batchc = batch.copy()\n",
    "    nj = batch.shape[0]\n",
    "    for i in range( nj ):\n",
    "        pts = batch[i,0,:]\n",
    "        etas = batch[i,1,:]\n",
    "        phis = batch[i,2,:]\n",
    "        nc = len( pts )\n",
    "        eta_shift = np.sum( [ pts[j]*etas[j] for j in range( nc ) ] ) / np.sum( pts )\n",
    "        phi_shift = np.sum( [ pts[j]*phis[j] for j in range( nc ) ] ) / np.sum( pts )\n",
    "        batchc[i,1,:] = batch[i,1,:] - eta_shift\n",
    "        batchc[i,2,:] = batch[i,2,:] - phi_shift\n",
    "    return batchc\n",
    "\n",
    "t = recentre_jet(x)\n",
    "n = recentre_jet_np(x.cpu().numpy())\n",
    "\n",
    "print(t.cpu().numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rescale_pt_np(dataset):\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        dataset[i,0,:] = dataset[i,0,:]/600\n",
    "    return dataset\n",
    "\n",
    "t = rescale_pts(x).cpu()\n",
    "n = rescale_pt_np(x.cpu().numpy())\n",
    "\n",
    "print(t.numpy() - n)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Constituents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constits_jet( batch, prob=0.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where each jet has some fraction of missing constituents\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batch_dropped = batch.clone()\n",
    "    #n_nonzero = torch.sum(batch_dropped[:,0,:]>0, dim=1)\n",
    "    nj = batch_dropped.shape[0]\n",
    "    nc = batch_dropped.shape[2]\n",
    "    torch.manual_seed(42)\n",
    "    mask = torch.rand((nj, nc)) > prob\n",
    "    print(\"first mask:\",mask)\n",
    "    mask = mask.int().to(device)\n",
    "    print(mask)\n",
    "    num_zeros_tensor = (mask == 0).sum().item()\n",
    "    #print(num_zeros_tensor)\n",
    "    batch_dropped = batch_dropped * mask.unsqueeze(1)\n",
    "\n",
    "    #print(batch_dropped)\n",
    "    pts = torch.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = torch.sum( batch_dropped[:,0,:], axis=1 )\n",
    "\n",
    "    pts_aug[pts_aug == 0] = 1\n",
    "    pt_rescale = pts/pts_aug\n",
    "\n",
    "    pTs= batch_dropped[:,0,:]\n",
    "    etas = batch_dropped[:,1,:]\n",
    "    phis = batch_dropped[:,2,:]\n",
    "    pTs *= pt_rescale.unsqueeze(1)\n",
    "    \n",
    "    batch_dropped = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "\n",
    "    #print(batch_dropped.size())\n",
    "    return recentre_jet( batch_dropped )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constits_jet_np( batch, prob=0.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where each jet has some fraction of missing constituents\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batchc = batch.copy()\n",
    "    nj = batchc.shape[0]\n",
    "    nc = batchc.shape[2]\n",
    "    nzs = np.array( [ np.where( batchc[:,0,:]>0.0 )[0].shape[0] for i in range(len(batch)) ] )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    mask = torch.rand((nj, nc)) > prob\n",
    "    print(\"first mask:\",mask)\n",
    "    mask = mask.int()\n",
    "    mask = mask.numpy()\n",
    "    print(mask)\n",
    "    num_zeros_array = ((np.array(mask)) == 0).sum()\n",
    "    #print(num_zeros_array)\n",
    "\n",
    "    for i in range( nj ):\n",
    "        for j in range( nc ):\n",
    "            if mask[i][j]==0:\n",
    "                batchc[i,:,j] = np.array([0.0,0.0,0.0])\n",
    "    pts = np.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = np.sum( batchc[:,0,:], axis=1 )\n",
    "    pt_rescale = [ pts[i]/pts_aug[i] for i in range(nj) ]\n",
    "    for i in range(nj):\n",
    "        batchc[i,0,:] = batchc[i,0,:]*pt_rescale[i]\n",
    "    return recentre_jet_np( batchc )\n",
    "\n",
    "x=x.to(device)\n",
    "t = drop_constits_jet(x,0.9)\n",
    "n = drop_constits_jet_np(x.cpu().numpy(),0.9)\n",
    "\n",
    "\n",
    "print(x.size())\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constits_jet_ordered( batch, prob=0.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where each jet has some fraction of missing constituents\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batch_dropped = batch.clone()\n",
    "    #n_nonzero = torch.sum(batch_dropped[:,0,:]>0, dim=1)\n",
    "    nj = batch_dropped.shape[0]\n",
    "    nc = batch_dropped.shape[2]\n",
    "    mask = torch.rand((nj, nc)) > prob\n",
    "    mask = mask.int().to(device)\n",
    "    batch_dropped = batch_dropped * mask.unsqueeze(1)\n",
    "\n",
    "    #print(batch_dropped)\n",
    "    pts = torch.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = torch.sum( batch_dropped[:,0,:], axis=1 )\n",
    "\n",
    "    pTs= batch_dropped[:,0,:]\n",
    "        \n",
    "    if torch.any(pts_aug != 0):\n",
    "        pt_rescale = torch.where(pts_aug != 0, pts / pts_aug, torch.ones_like(pts))\n",
    "        print(pt_rescale)\n",
    "        pTs *= pt_rescale.unsqueeze(1)\n",
    "\n",
    "\n",
    "    pTs, indices = torch.sort(pTs, dim=1, descending=True) # Ordering pTs\n",
    "    #print(\"pts:\", pTs)\n",
    "    etas = batch_dropped[:,1,:]\n",
    "    etas = etas.gather(dim=1,index=indices)\n",
    "    phis = batch_dropped[:,2,:]\n",
    "    phis = phis.gather(dim=1,index=indices)\n",
    "\n",
    "    \n",
    "    batch_dropped = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "    non_zero_count = torch.sum(pTs != 0, dim=-1, keepdim=True)\n",
    "    #print(batch_dropped)\n",
    "    if torch.min(non_zero_count)==0:\n",
    "        return drop_constits_jet_ordered(batch,prob)\n",
    "    else:\n",
    "        return recentre_jet( batch_dropped )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  1.4455e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00,  5.4634e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -0.0000e+00,  1.1028e-01,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  2.0434e+01,  1.5722e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -2.6525e-02, -1.5643e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -5.1978e-02, -4.5160e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.7668e+01,  1.2581e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  5.1081e-02,  9.1307e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  2.1180e-02,  3.9251e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3693e+01,  1.8498e+01,  1.0540e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.6171e-02, -4.0996e-02,  7.1618e-03,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-4.0308e-02,  1.0796e-01, -4.9008e-02,  ..., -0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 4.3447e+01,  3.4350e+01,  2.9348e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-2.7253e-03,  1.3967e-02, -1.6702e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-6.4128e-03,  8.3059e-03,  4.1052e-03,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.5172e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00, -2.9682e-03,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00, -2.9294e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "pTs= x[:,0,:]\n",
    "print(pTs.size())\n",
    "# Count non-zero entries in the last dimension\n",
    "non_zero_count = torch.sum(pTs != 0, dim=-1, keepdim=True)\n",
    "dropping_numbers = torch.round( non_zero_count * 0.3)\n",
    "#print(dropping_numbers)\n",
    "total_length_mask = pTs.size(1)\n",
    "\n",
    "mask_safe = []\n",
    "# Create a mask with zeros distributed between ones\n",
    "for i in range(len(dropping_numbers)):\n",
    "    length_of_nonzero_pTs = int(non_zero_count[i])\n",
    "    #print(dropping_numbers[i])\n",
    "    n_drop = int(dropping_numbers[i])\n",
    "\n",
    "    non_zero_mask = torch.cat((torch.zeros(n_drop), torch.ones(length_of_nonzero_pTs - n_drop))) #creating mask for non zero entries\n",
    "    shuffled_non_zero_mask = non_zero_mask[torch.randperm(non_zero_mask.size(0))]  # Generate random permutation of non-zero mask\n",
    "\n",
    "    #print(shuffled_non_zero_mask)\n",
    "\n",
    "    jet_mask = torch.cat((shuffled_non_zero_mask,torch.zeros(total_length_mask-length_of_nonzero_pTs))) #Creating mask for whole jet\n",
    "\n",
    "    mask_safe.append(jet_mask)\n",
    "\n",
    "mask = torch.stack(mask_safe)\n",
    "\n",
    "result = x.cpu() * mask.unsqueeze(1)\n",
    "print(result)\n",
    "    \n",
    "# Shuffle the mask tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constits_jet_safe( batch, prob=0.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where each jet has some fraction of missing constituents\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batch_dropped = batch.clone()\n",
    "\n",
    "    pTs= batch_dropped[:,0,:]\n",
    "    non_zero_count = torch.sum(pTs != 0, dim=-1, keepdim=True)\n",
    "    dropping_numbers = torch.round( non_zero_count * prob)\n",
    "    #print(dropping_numbers)\n",
    "    total_length_mask = pTs.size(1)\n",
    "\n",
    "    mask_safe = []\n",
    "    # Create a mask with zeros distributed between ones\n",
    "    for i in range(len(dropping_numbers)):\n",
    "        length_of_nonzero_pTs = int(non_zero_count[i])\n",
    "        #print(dropping_numbers[i])\n",
    "        n_drop = int(dropping_numbers[i])\n",
    "\n",
    "        non_zero_mask = torch.cat((torch.zeros(n_drop), torch.ones(length_of_nonzero_pTs - n_drop))) #creating mask for non zero entries\n",
    "        shuffled_non_zero_mask = non_zero_mask[torch.randperm(non_zero_mask.size(0))]  # Generate random permutation of non-zero mask\n",
    "\n",
    "        #print(shuffled_non_zero_mask)\n",
    "\n",
    "        jet_mask = torch.cat((shuffled_non_zero_mask,torch.zeros(total_length_mask-length_of_nonzero_pTs))) #Creating mask for whole jet\n",
    "\n",
    "        mask_safe.append(jet_mask)\n",
    "\n",
    "    mask = torch.stack(mask_safe).to(device)\n",
    "\n",
    "    result = x * mask.unsqueeze(1)\n",
    "\n",
    "    \n",
    "\n",
    "    #From here on just rescaling and reordering --> Masking is done here. \n",
    "\n",
    "    #print(batch_dropped)\n",
    "    pts = torch.sum( result[:,0,:], axis=1 )\n",
    "    pts_aug = torch.sum( result[:,0,:], axis=1 )\n",
    "\n",
    "\n",
    "    if torch.any(pts_aug != 0):\n",
    "        pt_rescale = torch.where(pts_aug != 0, pts / pts_aug, torch.ones_like(pts))\n",
    "        #print(pt_rescale)\n",
    "        pTs *= pt_rescale.unsqueeze(1)\n",
    "\n",
    "    pTs_dropped =result[:,0,:]\n",
    "\n",
    "    pTs, indices = torch.sort(pTs_dropped, dim=1, descending=True) # Ordering pTs\n",
    "    #print(\"pts:\", pTs)\n",
    "    etas = result[:,1,:]\n",
    "    etas = etas.gather(dim=1,index=indices)\n",
    "    phis = result[:,2,:]\n",
    "    phis = phis.gather(dim=1,index=indices)\n",
    "\n",
    "    \n",
    "    batch_dropped = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "\n",
    "    #print(batch_dropped)\n",
    "    return recentre_jet( batch_dropped )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8307e+01,  1.3657e+01,  1.0552e+01,  7.4996e+00,  7.3442e+00,\n",
      "          5.7860e+00,  5.6805e+00,  3.2277e+00,  2.5522e+00,  2.1958e+00,\n",
      "          1.8695e+00,  1.7832e+00,  1.4848e+00,  1.4705e+00,  1.3568e+00,\n",
      "          1.0411e+00,  1.0220e+00,  9.7170e-01,  9.4382e-01,  6.8864e-01,\n",
      "          6.4694e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-6.8864e-02, -5.9721e-02, -2.5275e-02,  4.3302e-02, -3.1788e-02,\n",
      "          1.0998e-01, -5.9714e-02, -1.4244e-01,  3.2493e-02,  1.2836e-01,\n",
      "         -5.5845e-02,  1.2249e-01, -6.6407e-01,  3.6923e-01, -2.5517e-02,\n",
      "          2.6570e-01, -1.2463e-01,  7.6234e-02,  3.1339e-03,  4.3478e-01,\n",
      "          1.1485e-01, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04,\n",
      "         -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04,\n",
      "         -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04,\n",
      "         -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04,\n",
      "         -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04,\n",
      "         -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04, -6.3625e-04],\n",
      "        [-2.1130e-01,  2.2232e-01,  5.3772e-02,  1.9274e-01, -1.5253e-01,\n",
      "         -3.2366e-01, -1.4577e-01, -2.0087e-01,  1.5928e-01,  7.5585e-02,\n",
      "          8.5989e-02, -5.1087e-01,  4.0160e-01, -2.1258e-01, -1.7243e-01,\n",
      "         -3.2794e-02,  9.4749e-03, -2.8308e-01,  1.3822e-01, -8.3906e-02,\n",
      "          4.5601e-01,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,\n",
      "          1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,\n",
      "          1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,\n",
      "          1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,\n",
      "          1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,\n",
      "          1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02,  1.7921e-02]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1.8871e+01,  1.8307e+01,  1.4455e+01,  1.3657e+01,  1.0552e+01,\n",
      "          9.3065e+00,  8.9446e+00,  7.4996e+00,  7.3442e+00,  6.8299e+00,\n",
      "          5.9946e+00,  5.8821e+00,  5.7860e+00,  5.7517e+00,  5.6805e+00,\n",
      "          4.8588e+00,  3.6905e+00,  3.3763e+00,  3.2277e+00,  3.0599e+00,\n",
      "          2.6965e+00,  2.5832e+00,  2.5522e+00,  2.4475e+00,  2.3646e+00,\n",
      "          2.1958e+00,  1.8695e+00,  1.7832e+00,  1.4848e+00,  1.4705e+00,\n",
      "          1.3568e+00,  1.2770e+00,  1.2469e+00,  1.0411e+00,  1.0220e+00,\n",
      "          9.7170e-01,  9.4382e-01,  8.8641e-01,  7.9258e-01,  6.8864e-01,\n",
      "          6.4694e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-4.8892e-03, -6.8228e-02,  5.4634e-02, -5.9085e-02, -2.4639e-02,\n",
      "          4.0998e-02, -1.0387e-02,  4.3938e-02, -3.1152e-02, -1.1256e-02,\n",
      "         -3.1079e-02,  8.4251e-02,  1.1061e-01, -6.0162e-02, -5.9078e-02,\n",
      "         -2.0927e-02, -6.8533e-02,  4.5708e-02, -1.4180e-01, -4.4793e-02,\n",
      "         -3.8318e-04,  5.6949e-02,  3.3129e-02, -3.7523e-02,  2.7253e-01,\n",
      "          1.2900e-01, -5.5209e-02,  1.2312e-01, -6.6344e-01,  3.6986e-01,\n",
      "         -2.4881e-02,  1.5029e-01, -2.4771e-01,  2.6634e-01, -1.2399e-01,\n",
      "          7.6870e-02,  3.7702e-03,  5.6793e-01,  1.7125e-01,  4.3542e-01,\n",
      "          1.1549e-01,  3.7173e+00,  3.7173e+00,  3.7173e+00,  3.7173e+00,\n",
      "          3.7173e+00,  3.7173e+00,  3.7173e+00,  3.7173e+00,  3.7173e+00],\n",
      "        [ 2.2933e-01, -2.2922e-01,  1.1028e-01,  2.0440e-01,  3.5851e-02,\n",
      "          1.5694e-01,  2.2284e-01,  1.7482e-01, -1.7045e-01, -3.2755e-01,\n",
      "         -1.9401e-01,  2.1982e-01, -3.4158e-01, -8.8831e-02, -1.6370e-01,\n",
      "         -2.3153e-01, -1.8276e-01,  1.3891e-01, -2.1879e-01, -1.1041e-02,\n",
      "         -2.1325e-03,  8.2104e-02,  1.4136e-01, -2.9799e-01, -2.1650e-01,\n",
      "          5.7664e-02,  6.8067e-02, -5.2879e-01,  3.8368e-01, -2.3050e-01,\n",
      "         -1.9035e-01,  1.1594e-02,  1.1649e-01, -5.0715e-02, -8.4461e-03,\n",
      "         -3.0100e-01,  1.2030e-01,  3.9351e-01,  1.2047e-01, -1.0183e-01,\n",
      "          4.3809e-01, -3.1090e+00, -3.1090e+00, -3.1090e+00, -3.1090e+00,\n",
      "         -3.1090e+00, -3.1090e+00, -3.1090e+00, -3.1090e+00, -3.1090e+00]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(drop_constits_jet_safe(x)[0])\n",
    "print(x[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pT reweight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_reweight_jet( batch, beta=1.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where the pt of the constituents in each jet has has been re-weighted by some power\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batchc = batch.clone()\n",
    "\n",
    "    etas = batchc[:,1,:]\n",
    "    phis = batchc[:,2,:]\n",
    "    batchc = batchc[:,0,:]**beta\n",
    "    pts = torch.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = torch.sum( batchc, axis=1 )\n",
    "\n",
    "    pts_aug[pts_aug == 0] = 1\n",
    "    pt_rescale =  pts/pts_aug\n",
    "    pTs = pt_rescale.unsqueeze(-1)* batchc\n",
    "    #print(pTs)\n",
    "\n",
    "    jet = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "    return recentre_jet( jet.float() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_reweight_jet_np( batch, beta=1.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where the pt of the constituents in each jet has has been re-weighted by some power\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batchc = batch.copy()\n",
    "    nj = batchc.shape[0]\n",
    "    nc = batchc.shape[2]\n",
    "    for i in range( nj ):\n",
    "        for j in range( nc ):\n",
    "            batchc[i,0,j] = batch[i,0,j]**beta\n",
    "    pts = np.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = np.sum( batchc[:,0,:], axis=1 )\n",
    "    pt_rescale = [ pts[i]/pts_aug[i] for i in range(nj) ]\n",
    "    for i in range(nj):\n",
    "        batchc[i,0,:] = batchc[i,0,:]*pt_rescale[i]\n",
    "    #print(batchc[:,0,:])\n",
    "    return recentre_jet_np( batchc )\n",
    "\n",
    "t = pt_reweight_jet(x)\n",
    "n = pt_reweight_jet_np(x.cpu().numpy())\n",
    "\n",
    "print(t.cpu().numpy() - n)\n",
    "print(t - torch.Tensor(n).to(device))\n",
    "#print(t - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_reweight_jet_ordered( batch, beta=1.5 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    Dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets where the pt of the constituents in each jet has has been re-weighted by some power\n",
    "    Note: rescale pts so that the augmented jet pt matches the original\n",
    "    '''\n",
    "    batchc = batch.clone()\n",
    "\n",
    "    etas = batchc[:,1,:]\n",
    "    phis = batchc[:,2,:]\n",
    "    batchc = batchc[:,0,:]**beta\n",
    "    pts = torch.sum( batch[:,0,:], axis=1 )\n",
    "    pts_aug = torch.sum( batchc, axis=1 )\n",
    "\n",
    "    pts_aug[pts_aug == 0] = 1\n",
    "    pt_rescale =  pts/pts_aug\n",
    "    pTs = pt_rescale.unsqueeze(-1)* batchc\n",
    "    #print(pTs)\n",
    "    pTs, indices = torch.sort(pTs, dim=1, descending=True) # Ordering pTs\n",
    "    etas = batch_dropped[:,1,:]\n",
    "    etas = etas.gather(dim=1,index=indices)\n",
    "    phis = batch_dropped[:,2,:]\n",
    "    phis = phis.gather(dim=1,index=indices)\n",
    "\n",
    "    jet = torch.cat((pTs.unsqueeze(1),etas.unsqueeze(1),phis.unsqueeze(1)),axis = 1)\n",
    "    return recentre_jet( jet.float() )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jets(batch):\n",
    "    batch_filled = batch.clone()\n",
    "    n_constit = batch_filled.shape[2]\n",
    "    n_nonzero = torch.sum(batch_filled[:,0,:]>0, dim=1) #number of zero constituents\n",
    "    print(n_nonzero)\n",
    "    n_split = torch.min(torch.stack([n_nonzero, n_constit-n_nonzero], dim=1), dim=1).values\n",
    "    print(n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = add_jets(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical Augmentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinear fill jets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_fill_jets_fast(batch):\n",
    "    '''\n",
    "    Fill as many of the zero-padded entries with collinear splittings\n",
    "    of the constituents by splitting each constituent at most once.\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : torch.Tensor\n",
    "        batch of jets with zero-padding\n",
    "    Returns\n",
    "    -------\n",
    "    batch_filled : torch.Tensor\n",
    "        batch of jets with collinear splittings\n",
    "    '''\n",
    "    batch_filled = batch.clone()\n",
    "    n_constit = batch_filled.shape[2]\n",
    "    n_nonzero = torch.sum(batch_filled[:,0,:]>0, dim=1)\n",
    "    \n",
    "    n_split = torch.min(torch.stack([n_nonzero, n_constit-n_nonzero], dim=1), dim=1).values\n",
    "    idx_flip = torch.where(n_nonzero != n_split)[0]\n",
    "    mask_split = (batch_filled[:,0,:] != 0)\n",
    "    \n",
    "    mask_split[idx_flip] = torch.flip(mask_split[idx_flip].float(), dims=[1]).bool()\n",
    "\n",
    "    #print(mask_split)\n",
    "    mask_split[idx_flip] = ~mask_split[idx_flip]\n",
    "    r_split = torch.rand(size=mask_split.shape, device=batch.device)\n",
    "    \n",
    "    a = r_split*mask_split*batch_filled[:,0,:]\n",
    "    b = (1-r_split)*mask_split*batch_filled[:,0,:]\n",
    "    c = ~mask_split*batch_filled[:,0,:]\n",
    "    batch_filled[:,0,:] = a+c+torch.flip(b, dims=[1])\n",
    "    batch_filled[:,1,:] += torch.flip(mask_split*batch_filled[:,1,:], dims=[1])\n",
    "    batch_filled[:,2,:] += torch.flip(mask_split*batch_filled[:,2,:], dims=[1])\n",
    "    return batch_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_fill_jets_np( batch ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets with collinear splittings, the function attempts to fill as many of the zero-padded args.nconstit\n",
    "    entries with collinear splittings of the constituents by splitting each constituent at most once, same shape as input\n",
    "    '''\n",
    "    batchb = batch.copy()\n",
    "    nc = batch.shape[2]\n",
    "    nzs = np.array( [ np.where( batch[:,0,:][i]>0.0)[0].shape[0] for i in range(len(batch)) ] )\n",
    "    for k in range(len(batch)):\n",
    "        nzs1 = np.max( [ nzs[k], int(nc/2) ] )\n",
    "        zs1 = int(nc-nzs1)\n",
    "        els = np.random.choice( np.linspace(0,nzs1-1,nzs1), size=zs1, replace=False )\n",
    "        rs = np.random.uniform( size=zs1 )\n",
    "        for j in range(zs1):\n",
    "            batchb[k,0,int(els[j])] = rs[j]*batch[k,0,int(els[j])]\n",
    "            batchb[k,0,int(nzs[k]+j)] = (1-rs[j])*batch[k,0,int(els[j])]\n",
    "            batchb[k,1,int(nzs[k]+j)] = batch[k,1,int(els[j])]\n",
    "            batchb[k,2,int(nzs[k]+j)] = batch[k,2,int(els[j])]\n",
    "    return batchb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = collinear_fill_jets_fast(x)\n",
    "n = collinear_fill_jets_np(x.cpu().numpy())\n",
    "\n",
    "print(t.cpu().numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_jets(batch):\n",
    "\n",
    "    rot_batch = batch # is this right when it should stay on gpu?\n",
    "    batch_size = batch.size(0)\n",
    "    constit = batch.size(2)\n",
    "\n",
    "    rotate_tensor = torch.rand([batch_size,constit]) * 2 * np.pi #creating the array of random rotations\n",
    "\n",
    "    rot_batch[:,2,:] =+ np.pi # shifting the phi tensor to make use of the % function\n",
    "    rot_batch[:,2,:] += rotate_tensor\n",
    "    rot_batch[:,2,:] %= 2 * np.pi # getting it in the same output range\n",
    "    rot_batch[:,2,:] =- np.pi # shifting back\n",
    "\n",
    "    return rot_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bary does it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def rotate_jets(batch, ra ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets rotated independently in eta-phi, same shape as input\n",
    "    '''\n",
    "    device = batch.device\n",
    "    batch_size = batch.size(0)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    rot_angle = ra\n",
    "    #rot_angle = torch.rand(batch_size, device=\"cpu\") * 2 * np.pi\n",
    "    c = torch.cos(rot_angle)\n",
    "    s = torch.sin(rot_angle)\n",
    "    o = torch.ones_like(rot_angle)\n",
    "    z = torch.zeros_like(rot_angle)\n",
    "\n",
    "    #print(o.shape)\n",
    "\n",
    "    rot_matrix = torch.stack([\n",
    "    torch.stack([o, z, z], dim=0),\n",
    "    torch.stack([z, c, s], dim=0),\n",
    "    torch.stack([z, -s, c], dim=0)], dim=1) # (3, 3, batch_size]\n",
    "\n",
    "    #print(rot_matrix[:,:,0])\n",
    "\n",
    "    return torch.einsum('ijk,lji->ilk', batch, rot_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_jets_np( batch , ra ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets rotated independently in eta-phi, same shape as input\n",
    "    '''\n",
    "\n",
    "    rot_angle = ra\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    #rot_angle = (torch.rand(128, device=\"cpu\") * 2 * np.pi).cpu().numpy()\n",
    "    c = np.cos(rot_angle)\n",
    "    s = np.sin(rot_angle)\n",
    "    o = np.ones_like(rot_angle)\n",
    "    z = np.zeros_like(rot_angle)\n",
    "    rot_matrix = np.array([[o, z, z], [z, c, -s], [z, s, c]]) # (3, 3, batchsize)\n",
    "    return np.einsum('ijk,lji->ilk', batch, rot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_angle = torch.rand(batch_size, device=\"cpu\") * 2 * np.pi\n",
    "\n",
    "t = rotate_jets(x.cpu(),rot_angle).cpu().numpy()\n",
    "n = rotate_jets_np(x.cpu().numpy(),rot_angle.numpy())\n",
    "\n",
    "print((t- n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "rot_angle = torch.ones(batch_size, device=\"cpu\")\n",
    "\n",
    "c = torch.Tensor([1])#torch.cos(rot_angle)\n",
    "s = torch.Tensor([2])#torch.sin(rot_angle)\n",
    "o = torch.Tensor([3])#torch.ones_like(rot_angle)\n",
    "z = torch.Tensor([4])#torch.zeros_like(rot_angle)\n",
    "\n",
    "rot_matrix = torch.stack([\n",
    "torch.stack([o, z, z], dim=0),\n",
    "torch.stack([z, c, s], dim=0),\n",
    "torch.stack([z, -s, c], dim=0)], dim=1) # (3, 3, batch_size]\n",
    "\n",
    "rot_matrix_np = np.array([[o.numpy(), z.numpy(), z.numpy()], [z.numpy(), c.numpy(), -s.numpy()], [z.numpy(), s.numpy(), c.numpy()]]) # (3, 3, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rot_matrix.numpy())\n",
    "print(rot_matrix_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distort Jets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def distort_jets(batch, strength=0.1, pT_clip_min=0.1):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets with each constituents position shifted independently, shifts drawn from normal with mean 0, std strength/pT, same shape as input\n",
    "    '''\n",
    "    #strength = torch.Tensor(strength).to(device)\n",
    "    pT = batch[:, 0]  # (batchsize, n_constit)\n",
    "    shift_eta = torch.nan_to_num(\n",
    "        #strength * torch.randn(batch.shape[0], batch.shape[2]).to(device) / pT.clip(min=pT_clip_min).to(device),\n",
    "        strength * torch.ones(batch.shape[0], batch.shape[2]).to(device) / pT.clip(min=pT_clip_min).to(device),\n",
    "        posinf=0.0,\n",
    "        neginf=0.0,\n",
    "    ).to(device)  # * mask\n",
    "    shift_phi = torch.nan_to_num(\n",
    "        #strength * torch.randn(batch.shape[0], batch.shape[2]).to(device) / pT.clip(min=pT_clip_min).to(device),\n",
    "        strength * torch.ones(batch.shape[0], batch.shape[2]).to(device) / pT.clip(min=pT_clip_min).to(device),\n",
    "        posinf=0.0,\n",
    "        neginf=0.0,\n",
    "    ).to(device)  # * mask\n",
    "    shift = torch.stack([torch.zeros((batch.shape[0], batch.shape[2])).to(device), shift_eta, shift_phi], 1)\n",
    "    return batch + shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_jets_np( batch, strength=0.1, pT_clip_min=0.1 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets with each constituents position shifted independently, shifts drawn from normal with mean 0, std strength/pT, same shape as input\n",
    "    '''\n",
    "    pT = batch[:,0]   # (batchsize, n_constit)\n",
    "    #shift_eta = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
    "    #shift_phi = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
    "\n",
    "    shift_eta = np.nan_to_num(strength * np.ones((batch.shape[0], batch.shape[2]), dtype=np.float64) / pT.clip(min=pT_clip_min), posinf=0.0, neginf=0.0)\n",
    "    shift_phi = np.nan_to_num(strength * np.ones((batch.shape[0], batch.shape[2]), dtype=np.float64) / pT.clip(min=pT_clip_min), posinf=0.0, neginf=0.0)\n",
    "\n",
    "    shift = np.stack( [ np.zeros( (batch.shape[0], batch.shape[2]) ), shift_eta, shift_phi ], 1)\n",
    "    return batch + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = distort_jets(x).cpu()\n",
    "n = distort_jets_np(x.cpu().numpy())\n",
    "\n",
    "print(t.numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Jets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ptp(input, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        return input.max() - input.min()\n",
    "    return input.max(dim, keepdim).values - input.min(dim, keepdim).values\n",
    "\n",
    "\n",
    "def translate_jets(batch, width=1.0):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of eta-phi translated jets, same shape as input\n",
    "    '''\n",
    "\n",
    "    device = batch.device\n",
    "    mask = (batch[:, 0] > 0)  # 1 for constituents with non-zero pT, 0 otherwise\n",
    "    ptp_eta = ptp(batch[:, 1, :], dim=-1, keepdim=True)  # ptp = 'peak to peak' = max - min DOUBLE CHECKED\n",
    "    ptp_phi = ptp(batch[:, 2, :], dim=-1, keepdim=True)  # ptp = 'peak to peak' = max - min\n",
    "    low_eta = -width * ptp_eta\n",
    "    high_eta = +width * ptp_eta\n",
    "    low_phi = torch.max(-width * ptp_phi, -torch.tensor(np.pi, device=device) - torch.amin(batch[:, 2, :], dim=-1, keepdim=True))\n",
    "    high_phi = torch.min(+width * ptp_phi, +torch.tensor(np.pi, device=device) - torch.amax(batch[:, 2, :], dim=-1, keepdim=True)) #DOUBLE CHECKED\n",
    "    shift_eta = mask * torch.rand((batch.shape[0], 1), device=device) * (high_eta - low_eta) + low_eta\n",
    "    shift_phi = mask * torch.rand((batch.shape[0], 1), device=device) * (high_phi - low_phi) + low_phi\n",
    "    shift = torch.stack([torch.zeros((batch.shape[0], batch.shape[2]), device=device), shift_eta, shift_phi], dim=1)\n",
    "    print(shift)\n",
    "    shifted_batch = batch + shift\n",
    "    return shifted_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_jets_np( batch, width=1.0 ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of eta-phi translated jets, same shape as input\n",
    "    '''\n",
    "    \n",
    "    mask = (batch[:,0] > 0) # 1 for constituents with non-zero pT, 0 otherwise\n",
    "    ptp_eta  = np.ptp(batch[:,1,:], axis=-1, keepdims=True) # ptp = 'peak to peak' = max - min\n",
    "    ptp_phi  = np.ptp(batch[:,2,:], axis=-1, keepdims=True) # ptp = 'peak to peak' = max - min\n",
    "    low_eta  = -width*ptp_eta\n",
    "    high_eta = +width*ptp_eta\n",
    "    low_phi  = np.maximum(-width*ptp_phi, -np.pi-np.amin(batch[:,2,:], axis=1).reshape(ptp_phi.shape))\n",
    "    high_phi = np.minimum(+width*ptp_phi, +np.pi-np.amax(batch[:,2,:], axis=1).reshape(ptp_phi.shape))\n",
    "    print(high_phi)\n",
    "    #shift_eta = (mask * torch.rand((batch.shape[0], 1), device=device) * (high_eta - low_eta)).cpu().numpy()\n",
    "    #shift_phi = (mask * torch.rand((batch.shape[0], 1), device=device) * (high_phi - low_phi)).cpu().numpy()\n",
    "\n",
    "    shift_eta = mask*np.random.uniform(low=low_eta, high=high_eta, size=(batch.shape[0], 1))\n",
    "    shift_phi = mask*np.random.uniform(low=low_phi, high=high_phi, size=(batch.shape[0], 1))\n",
    "    shift = np.stack([np.zeros((batch.shape[0], batch.shape[2])), shift_eta, shift_phi], 1)\n",
    "    print(shift)\n",
    "    shifted_batch = batch+shift\n",
    "    return shifted_batch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the random function can not be compared i show here that they are doing the same using the mean and the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x\n",
    "ptp_eta = ptp(batch[:, 1, :], dim=-1, keepdim=True) \n",
    "batch = batch.cpu()\n",
    "ptp_eta_np  = np.ptp(batch[:,1,:], axis=-1, keepdims=True) # ptp = 'peak to peak' = max - min\n",
    "\n",
    "print(ptp_eta)\n",
    "print(ptp_eta_np)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters for uniform distribution\n",
    "low = 0.0\n",
    "high = 1.0\n",
    "size = (200, 3000)  # Shape of the output array\n",
    "\n",
    "# Generate random numbers using np.random.uniform()\n",
    "np_uniform = np.random.uniform(low=low, high=high, size=size)\n",
    "print(\"NumPy random numbers:\\n\", np_uniform)\n",
    "print(np.mean(np_uniform),np.var(np_uniform))\n",
    "# Generate random numbers using torch.rand()\n",
    "torch_uniform = low + (high - low) * torch.rand(size)\n",
    "print(\"PyTorch random numbers:\\n\", torch_uniform)\n",
    "print(torch.mean(torch_uniform),torch.var(torch_uniform))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_pts(batch):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of pT-normalised jets, pT in each jet sums to 1, same shape as input\n",
    "    '''\n",
    "    batch_norm = batch.clone()\n",
    "    batch_norm[:, 0, :] = torch.nan_to_num(batch_norm[:, 0, :] / torch.sum(batch_norm[:, 0, :], dim=1)[:, None], posinf=0.0, neginf=0.0)\n",
    "    return batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_pts_np( batch ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of pT-normalised jets, pT in each jet sums to 1, same shape as input\n",
    "    '''\n",
    "    batch_norm = batch.copy()\n",
    "    batch_norm[:,0,:] = np.nan_to_num(batch_norm[:,0,:]/np.sum(batch_norm[:,0,:], axis=1)[:, np.newaxis], posinf = 0.0, neginf = 0.0 )\n",
    "    return batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = normalise_pts(x).cpu()\n",
    "n = normalise_pts_np(x.cpu().numpy())\n",
    "\n",
    "print(t.numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_pts(batch):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of pT-rescaled jets, each constituent pT is rescaled by 600, same shape as input\n",
    "    '''\n",
    "    batch_rscl = batch.clone()\n",
    "    batch_rscl[:,0,:] = torch.nan_to_num(batch_rscl[:,0,:]/600, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return batch_rscl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_pts_np( batch ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of pT-rescaled jets, each constituent pT is rescaled by 600, same shape as input\n",
    "    '''\n",
    "    batch_rscl = batch.copy()\n",
    "    batch_rscl[:,0,:] = np.nan_to_num(batch_rscl[:,0,:]/600, posinf = 0.0, neginf = 0.0 )\n",
    "    return batch_rscl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rescale_pts(x).cpu()\n",
    "n = rescale_pts_np(x.cpu().numpy())\n",
    "\n",
    "\n",
    "print(t)\n",
    "print(t.numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_jets( batch, nc ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of cropped jets, each jet is cropped to nc constituents, shape (batchsize, 3, nc)\n",
    "    '''\n",
    "    batch_crop = batch.clone()\n",
    "    return batch_crop[:,:,0:nc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_jets_np( batch, nc ):\n",
    "    '''\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of cropped jets, each jet is cropped to nc constituents, shape (batchsize, 3, nc)\n",
    "    '''\n",
    "    batch_crop = batch.copy()\n",
    "    return batch_crop[:,:,0:nc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = crop_jets(x,3).cpu()\n",
    "n = crop_jets_np(x.cpu().numpy(),3)\n",
    "\n",
    "print(t.numpy() - n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
